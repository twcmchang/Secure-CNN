{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A pure TensorFlow implementation of a neural network. This can be\n",
    "used as a drop-in replacement for a Keras model.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from cleverhans.model import Model\n",
    "\n",
    "\n",
    "class MLP(Model):\n",
    "    \"\"\"\n",
    "    An example of a bare bones multilayer perceptron (MLP) class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, input_shape, pretrain_dict=None):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layer_names = []\n",
    "        self.layers = layers\n",
    "        self.input_shape = input_shape\n",
    "        if isinstance(layers[-1], Softmax):\n",
    "            layers[-1].name = 'probs'\n",
    "            layers[-2].name = 'logits'\n",
    "        else:\n",
    "            layers[-1].name = 'logits'\n",
    "            \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if hasattr(layer, 'name'):\n",
    "                name = layer.name\n",
    "                if (pretrain_dict is not None) and (name in pretrain_dict.keys()):\n",
    "                    layer.set_input_shape(input_shape, pretrain = pretrain_dict[name])\n",
    "                else:\n",
    "                    layer.set_input_shape(input_shape)\n",
    "            else:\n",
    "                name = layer.__class__.__name__ + str(i)\n",
    "                layer.name = name\n",
    "                layer.set_input_shape(input_shape)\n",
    "                \n",
    "            self.layer_names.append(name)\n",
    "            input_shape = layer.get_output_shape()\n",
    "\n",
    "    def fprop(self, x, set_ref=False):\n",
    "        states = []\n",
    "        for layer in self.layers:\n",
    "            if set_ref:\n",
    "                layer.ref = x\n",
    "            x = layer.fprop(x)\n",
    "            assert x is not None\n",
    "            states.append(x)\n",
    "        states = dict(zip(self.get_layer_names(), states))\n",
    "        return states\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "\n",
    "    def get_output_shape(self):\n",
    "        return self.output_shape\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, num_hid):\n",
    "        self.num_hid = num_hid\n",
    "\n",
    "    def set_input_shape(self, input_shape, pretrain=None):\n",
    "        batch_size, dim = input_shape\n",
    "        self.input_shape = [batch_size, dim]\n",
    "        self.output_shape = [batch_size, self.num_hid]\n",
    "        if pretrain is None:\n",
    "            init = tf.random_normal([dim, self.num_hid], dtype=tf.float32)\n",
    "            init = init / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(init), axis=0, keep_dims=True))\n",
    "            self.W = tf.Variable(init)\n",
    "            self.b = tf.Variable(np.zeros((self.num_hid,)).astype('float32'))\n",
    "        else:\n",
    "            self.W = tf.Variable(initial_value = pretrain[0], dtype = tf.float32)\n",
    "            self.b = tf.Variable(initial_value = pretrain[1], dtype = tf.float32)\n",
    "\n",
    "    def fprop(self, x):\n",
    "        return tf.matmul(x, self.W) + self.b\n",
    "\n",
    "\n",
    "class Conv2D(Layer):\n",
    "\n",
    "    def __init__(self, output_channels, kernel_shape, strides, padding, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.__dict__.update(locals())\n",
    "        del self.self\n",
    "\n",
    "    def set_input_shape(self, input_shape, pretrain=None):\n",
    "        batch_size, rows, cols, input_channels = input_shape\n",
    "        kernel_shape = tuple(self.kernel_shape) + (input_channels,\n",
    "                                                   self.output_channels)\n",
    "        assert len(kernel_shape) == 4\n",
    "        assert all(isinstance(e, int) for e in kernel_shape), kernel_shape\n",
    "        \n",
    "        if pretrain is None:\n",
    "            init = tf.random_normal(kernel_shape, dtype=tf.float32)\n",
    "            init = init / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(init), axis=(0, 1, 2)))\n",
    "            self.kernels = tf.Variable(init)\n",
    "            self.b = tf.Variable(np.zeros((self.output_channels,)).astype('float32'))\n",
    "        else:\n",
    "            self.kernels = tf.Variable(initial_value = pretrain[0], dtype = tf.float32)\n",
    "            self.b = tf.Variable(initial_value = pretrain[1], dtype = tf.float32)\n",
    "        \n",
    "        input_shape = list(input_shape)\n",
    "        input_shape[0] = 1\n",
    "        dummy_batch = tf.zeros(input_shape)\n",
    "        dummy_output = self.fprop(dummy_batch)\n",
    "        output_shape = [int(e) for e in dummy_output.get_shape()]\n",
    "        output_shape[0] = batch_size\n",
    "        self.output_shape = tuple(output_shape)\n",
    "\n",
    "    def fprop(self, x, dp = 1.0):\n",
    "        \"\"\"\n",
    "        allow different (1) dp and (2) bn \n",
    "        \"\"\"\n",
    "        \n",
    "        return tf.nn.conv2d(x, self.kernels, (1,) + tuple(self.strides) + (1,),\n",
    "                            self.padding) + self.b\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape\n",
    "        self.output_shape = shape\n",
    "\n",
    "    def fprop(self, x):\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape\n",
    "        self.output_shape = shape\n",
    "\n",
    "    def fprop(self, x):\n",
    "        return tf.nn.softmax(x)\n",
    "    \n",
    "class MaxPool(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def set_input_shape(self, shape):\n",
    "        batch_size, rows, cols, input_channels = shape\n",
    "        input_shape = list(shape)\n",
    "        input_shape[0] = 1\n",
    "        dummy_batch = tf.zeros(input_shape)\n",
    "        dummy_output = self.fprop(dummy_batch)\n",
    "        output_shape = [int(e) for e in dummy_output.get_shape()]\n",
    "        output_shape[0] = batch_size\n",
    "        \n",
    "        self.input_shape = shape\n",
    "        self.output_shape = tuple(output_shape)\n",
    "        \n",
    "    def fprop(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "class Flatten(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape\n",
    "        output_width = 1\n",
    "        for factor in shape[1:]:\n",
    "            output_width *= factor\n",
    "        self.output_width = output_width\n",
    "        self.output_shape = [shape[0], output_width]\n",
    "\n",
    "    def fprop(self, x):\n",
    "        return tf.reshape(x, [-1, self.output_width])\n",
    "\n",
    "\n",
    "def make_basic_cnn(nb_filters=64, nb_classes=10,\n",
    "                   input_shape=(None, 28, 28, 1)):\n",
    "    layers = [Conv2D(nb_filters, (8, 8), (2, 2), \"SAME\", name='conv1'),\n",
    "              ReLU(),\n",
    "              Conv2D(nb_filters * 2, (6, 6), (2, 2), \"VALID\"),\n",
    "              ReLU(),\n",
    "              Conv2D(nb_filters * 2, (5, 5), (1, 1), \"VALID\"),\n",
    "              ReLU(),\n",
    "              Flatten(),\n",
    "              Linear(nb_classes),\n",
    "              Softmax()]\n",
    "\n",
    "    model = MLP(layers, input_shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_basic_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1',\n",
       " 'ReLU1',\n",
       " 'Conv2D2',\n",
       " 'ReLU3',\n",
       " 'Conv2D4',\n",
       " 'ReLU5',\n",
       " 'Flatten6',\n",
       " 'logits',\n",
       " 'probs']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idp_conv_bn_layer(self, bottom, name, dp=1.0):\n",
    "        with tf.name_scope(name+str(int(dp*100))):\n",
    "            with tf.variable_scope(\"VGG16\",reuse=True):\n",
    "                conv_filter = tf.get_variable(name=name+\"_W\")\n",
    "                conv_biases = tf.get_variable(name=name+\"_b\")\n",
    "                conv_gamma  = tf.get_variable(name=name+\"_gamma\")\n",
    "                moving_mean = tf.get_variable(name=name+'_bn_mean')\n",
    "                moving_variance = tf.get_variable(name=name+'_bn_variance')\n",
    "                beta = tf.get_variable(name=name+'_beta')\n",
    "            H,W,C,O = conv_filter.get_shape().as_list()\n",
    "            \n",
    "            # create a mask determined by the dot product percentage\n",
    "            n1 = int(O * dp)\n",
    "            n0 = O - n1\n",
    "            mask = tf.constant(value=np.append(np.ones(n1, dtype='float32'), np.zeros(n0, dtype='float32')), dtype=tf.float32)\n",
    "            conv_gamma = tf.multiply(conv_gamma, mask)\n",
    "            beta = tf.multiply(beta, mask)\n",
    "            \n",
    "            conv = tf.nn.conv2d(bottom, conv_filter, [1, 1, 1, 1], padding='SAME')\n",
    "            conv = tf.nn.bias_add(conv, conv_biases)\n",
    "\n",
    "            from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "            def mean_var_with_update():\n",
    "                mean, variance = tf.nn.moments(conv, [0,1,2], name='moments')\n",
    "                with tf.control_dependencies([assign_moving_average(moving_mean, mean, 0.9),\n",
    "                                              assign_moving_average(moving_variance, variance, 0.9)]):\n",
    "                    return tf.identity(mean), tf.identity(variance)\n",
    "\n",
    "            mean, variance = tf.cond(self.is_train, mean_var_with_update, lambda:(moving_mean, moving_variance))\n",
    "\n",
    "            conv = tf.nn.batch_normalization(conv, mean, variance, beta, conv_gamma, 1e-05)\n",
    "            relu = tf.nn.relu(conv)\n",
    "            \n",
    "            return relu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
